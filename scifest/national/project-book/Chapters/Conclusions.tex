\epigraph{``It is our choices, Harry, that show what we truly are, far more than our abilities."}{Albus Dumbledore}

The previous chapters have discussed the development of an open source implementation to improve numerical weather prediction through the utilisation of a neural network architecture. Although extremely time consuming, the importance of an open source implementation cannot be understated. It could potentially open up a field to a wider group, and often doesn't receive the media attention it rightfully deserves. 

Future applications of this work are extremely wide ranging, from usage in numerical weather prediction schemes that take in observational data collected from satellites, and ground stations in order to provide a picture of future weather events; to forming at starting point in future research of atmospheric phenomena. While, at the moment, it definitely would be inaccurate to say the software is ready for such usage, with future enhancements which I will touch on in a minute, it most certainly will.

\section{Looking Back}
To prove the hypothesis that `it is possible to train a recurrent neural network on an atmospheric reanalysis dataset based on data from the 15 years, that such a neural network captures crucial weather patterns, and can predict the future evolution of the atmosphere, and that such a machine learning model will ultimately improve numerical weather prediction in comparison to established physics-based models.', it was necessary to carry out a series of appropriate benchmarks.

\subsection{Analysis of Results}
This report hypothesises that it is possible to train a recurrent neural network on an atmospheric reanalysis dataset based on data from the 15 years, that such a neural network captures crucial weather patterns, and can predict the future evolution of the atmosphere, and that such a machine learning model will ultimately improve numerical weather prediction in comparison to established physics-based models.

This project has been significantly enhanced and improved since it was submitted into SciFest@College Online last May. To gain an understanding of the performance increase since that time, a comparison between the performance of the current model against the performance of the previous model is shown in section \ref{old_model}. Air temperature is the only parameter examined for this comparison, as the previous model did not incorporate geopotential. Concerning the two metrics, root mean squared error and mean absolute error, there has been a dramatic performance improvement. There has been a mean decrease of 52.5 \% in the root mean squared error values, and a mean decrease of 48.6 \% in the mean absolute error values; on average a 50.6 \% decrease in error metrics across the board. This demonstrates the continued improvement and enhancement of the models over the last few months; but, it also demonstrates that the performance of the software can still be improved drastically. The performance increase has not reached a plateau, which is extremely promising.

One of the key factors which led to the development of a machine learning model was the expected decrease in computational resources required to generate a forecast. While the initial training of the model was computationally burdensome, particularly with respect to memory, the assumption made at the start of this project holds once the model is trained. Once the model has trained, a performance increase of 3.59 times can be expected in comparison against a physics-based model of a similar resolution, the ECMWF IFS T42. It is also important to note that the benchmark of the software was run on a consumer-grade, MacBook Pro; while the benchmark of the ECMWF IFS T42 model was performed on a single XC40 node with 36 cores. Hence, a further increase in performance can be expected with a similar configuration.

With respect to the benchmarking outlined in chapter \ref{benchmarking_chapter}, the forecast system needs to beat the climatology forecast and the persistence forecast to be classified as useful. The benchmarks have demonstrated that the model can be generally regarded as useful, particularly on longer periods and in relation to air temperature, in particular, however, the models generally fail to beat well established physics-based models at this time. The model is significantly better at creating air temperature predictions and appears to suffer with geopotential predictions. The root mean squared error and mean squared error demonstrate that the model's air temperature becomes useful after approximately 24 hours of forecast time, with the model ultimately beating the ECMWF IFS T42 model after approximately 96 hours. The picture for geopotential is less rosy, with the root mean squared error and mean squared error demonstrating that the model's geopotential predictions become useful after approximately 96 hours of forecast time. An interesting point to note is that the error values initially are quite high, the error values appear to plateau. This may suggest that the model may be quite useful at generating climate forecasts. Concerning spatial awareness as measured by the anomaly correlation coefficient, both the mode's geopotential and air temperature predictions become useful after approximately 120 hours of forecast time. The spatial awareness of the model can be generally regarded as quite poor, it appears that the spatial aspect of a weather forecast was not captured by the long short-term memory cell. An interesting point to note again is that the anomaly correlation coefficient begins to increase after approximately 72 hours of forecast time. It appears the model has captured the long term climate trends while neglecting local weather trends. This is an unexpected result and needs to be investigated further. 

Hence, the hypothesis that was proposed has partially been proven and can be accepted, as such.

\subsection{Sources of Error}

Hence, the hypothesis that was proposed has partially been proven, however, there are a few areas which could have hindered the performance of the software or lead to a possible source of error:

\begin{itemize}
    \item As mentioned in section \ref{pca_section}, principal component analysis was used as a form of dimensionality reduction to reduce the number of features in the dataset, ultimately reducing the amount of memory required for the training phase of the project. For a given training window, there is 22,032,000 points. Attempting such a dataset would be computational burdensome, hence, the attractiveness of dimensionality reduction. During this process, 99\% of the variance was retained. While this maintains the majority of the important features, considering the atmosphere is a dynamical system, it is affected by chaos theory. Considering, chaos theory states that a small change in initial conditions can lead to dramatically different outcomes; this process of dimensionality reduction may be fundamentally hindering the possible performance of the software. 
    \item As mentioned previously, the benchmarking process was inspired by the open-source project, known as WeatherBench. WeatherBench proposes simple and clear evaluation metrics which will enable a direct comparison between different forecasting models\cite{rasp2020weatherbench}. This was done in order to conform with the meteorological and wider academic community. It be must be noted, however, that our current models predict values for five distinctive fields. As such, three fields were not rigorously analysed and studied. This has the potential of the models, in reality, either performing better than the results may suggest, or the limited results may be hiding a demon. 
    \item As mentioned in section \ref{era5_dataset}, it was decided to use a spatial resolution of $3^{\circ}$ ($60 \times 120$ grid points) and a temporal resolution of 2 hours. In regards to pressure surfaces, 17 vertical pressure levels were ultimately chosen:  1, 2, 5, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900,  950, 1000 hPa. A lower resolution was chosen in order to reduce the amount of computational resources required to train the model. Through high spatial resolution, however, a forecast can show the effects of local air currents, topography and soil cover. The forecasts produced thereby show local weather differences in more precise way\cite{res}. High resolution produces high precision, hence, while choosing a lower resolution may have lowered the computational burden during training, it may have had a significant on the performance of the model.
\end{itemize}

\section{Looking Ahead}
The software is currently in an alpha release state. An alpha version of any software is a very early version of the software that may not contain all of the features that are planned for the final version\cite{alpha}. In this section, I will briefly outline the enhancements and features that will be released in the beta version of the software, which is planned for release in Spring 2021:

\begin{itemize}
    \item As mentioned in section \ref{implement_rnn}, it is necessary to flatten the 3-dimensional vector that represents the state of the atmosphere in order for it to match the shape of the LSTM cell. While this is an effective approach, a new approach has been shown to be more effective. By combining a convolutional neural network with a neural network, it will result in a more accurate output for spatial-temporal type data. A convolutional neural network is a class of deep neural networks. They are also known as shift invariant or space invariant artificial neural networks, based on their shared-weights architecture and translation invariance characteristics. Considering the current model particularly suffers in the metric of the anomaly correlation coefficient, which is a spatial correlation between the forecast anomaly and the verifying analysis anomaly, a model which is built with spatial awareness may further improve performance. This will be investigated in the coming months. It may also reduce the need to use dimensionality reduction, as such a model may dramatically decrease the memory requirements to train the software. 
    \item As mentioned previously, a high resolution weather forecast produces high precision. As a result, in order to improve the performance of the model, the model will be trained on a higher resolution dataset. At this point, a resolution has not been decided, however, the decision will be made based on the computational resources available to initially train the model and the expected increase in performance that could be made by switching to a higher resolution. 
    \item The five parameters on which the machine learning models were trained upon were: air temperature, relative humidity, geopotential, zonal wind, and meridional wind. While these parameters are extremely important to predict from a meteorological point of view, the general public require predictions for the amount of precipitation to be made several days in advance; in order to make personal, and business decisions. This may be supplying shops with more food during periods of snowfall, or county councils setting up flood defences in town. In the coming months, our model will be trained on such parameters in order to provide the most useful weather forecast possible.  
\end{itemize}